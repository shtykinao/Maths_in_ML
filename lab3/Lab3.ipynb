{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyreadstat\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(path):\n",
    "    df, meta = pyreadstat.read_sav(path)\n",
    "\n",
    "    required_columns = ['yj1.1.1', 'yj13.2', 'yj72.18a', 'yj6.2', 'yj10.2', 'yj21b']\n",
    "\n",
    "    filtered_df = df[required_columns]\n",
    "    filtered_df = filtered_df.rename(columns={\n",
    "        'yj1.1.1': 'label',\n",
    "        'yj13.2': 'salary',\n",
    "        'yj72.18a': 'education',\n",
    "        'yj6.2': 'working_hours',\n",
    "        'yj10.2': 'bonus',\n",
    "        'yj21b': 'vacation_days'\n",
    "    })\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label   salary  education  working_hours       bonus  vacation_days\n",
      "0    NaN      NaN        4.0            NaN         NaN            NaN\n",
      "1    NaN      NaN        1.0            NaN         NaN            NaN\n",
      "2    NaN      NaN        6.0            NaN         NaN            NaN\n",
      "3    NaN      NaN        2.0            NaN         NaN            NaN\n",
      "4    3.0  25000.0        5.0           40.0  99999996.0           42.0\n"
     ]
    }
   ],
   "source": [
    "dataset = read_dataset('dataset.sav')\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()\n",
    "dataset = dataset[(dataset <= 1000000).all(axis=1)]\n",
    "dataset = dataset.astype(float)\n",
    "label = dataset['label']\n",
    "dataset = dataset.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Здесь можно универсально подать свои \n",
    "dataset[x,5]\n",
    "\n",
    "label[x] (где возможные ответы имеют значения от 1 до 5 дискретно)\n",
    "\n",
    "и запускать модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataset, label, test_size = 0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamOptimizer:\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.m_w, self.v_w = 0, 0  # Moments for weights\n",
    "        self.m_b, self.v_b = 0, 0  # Moments for biases\n",
    "        self.t = 0  # Time step\n",
    "    \n",
    "    def update(self, weights, biases, dW, db):\n",
    "        \"\"\"\n",
    "        Update weights and biases using Adam optimization.\n",
    "        \n",
    "        Parameters:\n",
    "        weights (ndarray): Current weights.\n",
    "        biases (ndarray): Current biases.\n",
    "        dW (ndarray): Gradients of the weights.\n",
    "        db (ndarray): Gradients of the biases.\n",
    "        \n",
    "        Returns:\n",
    "        updated_weights, updated_biases: Updated weights and biases.\n",
    "        \"\"\"\n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first moment estimate\n",
    "        self.m_w = self.beta1 * self.m_w + (1 - self.beta1) * dW\n",
    "        self.m_b = self.beta1 * self.m_b + (1 - self.beta1) * db\n",
    "        \n",
    "        # Update biased second moment estimate\n",
    "        self.v_w = self.beta2 * self.v_w + (1 - self.beta2) * (dW**2)\n",
    "        self.v_b = self.beta2 * self.v_b + (1 - self.beta2) * (db**2)\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_w_hat = self.m_w / (1 - self.beta1**self.t)\n",
    "        m_b_hat = self.m_b / (1 - self.beta1**self.t)\n",
    "        \n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_w_hat = self.v_w / (1 - self.beta2**self.t)\n",
    "        v_b_hat = self.v_b / (1 - self.beta2**self.t)\n",
    "        \n",
    "        # Compute the updates\n",
    "        weight_update = self.learning_rate * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "        bias_update = self.learning_rate * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "        \n",
    "        # Update parameters\n",
    "        weights -= weight_update\n",
    "        biases -= bias_update\n",
    "        \n",
    "        return weights, biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassLogisticRegression:\n",
    "    def __init__(self, num_features, num_classes, learning_rate=0.01, epochs=1000):\n",
    "        self.num_classes = num_classes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights = np.random.randn(num_features, num_classes) * 0.0001\n",
    "        self.weights = np.random.uniform(0, 0.01, (num_features, num_classes))\n",
    "        self.biases = np.zeros((1, num_classes))\n",
    "        self.optimizer = AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    def fit(self, X, y, optimizer = 'Naive'):\n",
    "        \"\"\"\n",
    "        Train the model using gradient descent.\n",
    "        \n",
    "        optimizer = ['Naive','Adam']\n",
    "        \"\"\"\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            # Compute linear scores\n",
    "            logits = np.dot(X, self.weights) + self.biases\n",
    "            \n",
    "            # Apply softmax\n",
    "            y_pred = self.softmax(logits)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.cross_entropy_loss(y, y_pred)\n",
    "\n",
    "            # Gradients\n",
    "            y_one_hot = self.one_hot_encode(y, self.num_classes)\n",
    "            dW = np.dot(X.T, (y_pred - y_one_hot)) / n_samples\n",
    "            db = np.sum(y_pred - y_one_hot, axis=0, keepdims=True) / n_samples\n",
    "\n",
    "            if optimizer == 'Adam':\n",
    "                self.weights, self.biases = self.optimizer.update(self.weights, self.biases, dW, db)\n",
    "            else:\n",
    "                self.weights -= self.learning_rate * dW\n",
    "                self.biases -= self.learning_rate * db\n",
    "\n",
    "            # Print loss\n",
    "            if epoch % 50 == 0:\n",
    "                print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    def softmax(self, x):\n",
    "        \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "        x -= x.max(axis=-1, keepdims=True)\n",
    "        e_x = np.exp(x)\n",
    "        result = e_x / e_x.sum(axis=-1, keepdims=True)\n",
    "        result = np.clip(result, 0.0001, 1.0)\n",
    "        return result\n",
    "\n",
    "    def cross_entropy_loss(self, y_true, y_pred):    \n",
    "        y_true = (y_true - 1).astype(int)\n",
    "        loss = 0\n",
    "\n",
    "        for i in range(len(y_pred)):\n",
    "            ans = y_pred[i][y_true.iloc[i]]\n",
    "            loss = loss + (-np.log(ans))\n",
    "        return loss\n",
    "\n",
    "    def one_hot_encode(self, y, num_classes):\n",
    "        \"\"\"One-hot encode target labels.\"\"\"\n",
    "        y = y.astype(int)\n",
    "        one_hot = np.zeros((y.size, num_classes))\n",
    "        for i in range(len(one_hot)):\n",
    "            one_hot[i][y.iloc[i]] = 1\n",
    "        return one_hot\n",
    "\n",
    "    def accuracy(self, y_true, y_pred):\n",
    "        correct_predictions = np.sum(y_true == y_pred)\n",
    "        return correct_predictions / len(y_true)\n",
    "\n",
    "    def predict(self, X, y=[]):\n",
    "        \"\"\"Predict class labels.\"\"\"\n",
    "        logits = np.dot(X, self.weights) + self.biases\n",
    "        probabilities = self.softmax(logits)\n",
    "        labels = np.argmax(probabilities, axis=1)\n",
    "\n",
    "        if y is not None:\n",
    "            accuracy = self.accuracy(y, labels)\n",
    "        else:\n",
    "            accuracy = []\n",
    "        return np.argmax(probabilities, axis=1), accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAIN VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features=5\n",
    "num_classes=5\n",
    "learning_rate=1e-5\n",
    "epochs=1000\n",
    "optimizer='Adam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1971.0128\n",
      "Epoch 50, Loss: 1953.3019\n",
      "Epoch 100, Loss: 1913.3491\n",
      "Epoch 150, Loss: 1683.8528\n",
      "Epoch 200, Loss: 1602.3631\n",
      "Epoch 250, Loss: 1471.9838\n",
      "Epoch 300, Loss: 1036.5992\n",
      "Epoch 350, Loss: 889.3305\n",
      "Epoch 400, Loss: 843.0330\n",
      "Epoch 450, Loss: 813.3640\n",
      "Epoch 500, Loss: 789.5537\n",
      "Epoch 550, Loss: 753.1008\n",
      "Epoch 600, Loss: 738.1937\n",
      "Epoch 650, Loss: 734.1018\n",
      "Epoch 700, Loss: 730.7521\n",
      "Epoch 750, Loss: 729.0688\n",
      "Epoch 800, Loss: 728.4838\n",
      "Epoch 850, Loss: 728.2331\n",
      "Epoch 900, Loss: 728.2709\n",
      "Epoch 950, Loss: 728.6434\n",
      "Accuracy: 0.43283582089552236\n",
      "Predictions: [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(89)\n",
    "# Initialize and train model\n",
    "model = MulticlassLogisticRegression(num_features=num_features, num_classes=num_classes, learning_rate=learning_rate, epochs=epochs)\n",
    "model.fit(X_train, y_train, optimizer=optimizer)\n",
    "\n",
    "predictions, accuracy = model.predict(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Predictions:\", predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
